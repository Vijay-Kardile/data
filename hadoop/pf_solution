PF - Solution (POC) 
********************************************
Source to HDFS Staging: 
- C:D to hadoop file gateway/landing area
- Credit Card Data, Mortgage Data gets ingested into hadoop staging area
- files are pushed to HDFS using WebHDFS put API
e.g.curl -i -X PUT -L "http://$<Host_Name>:$<Port>/webhdfs/v1/foo/newFile?op=CREATE" -T newFile
- file validation in HDFS
- jobs are scheduled in TWS
********************************************
HDFS Staging to Hive : 
-  Validation will be done by Falcon job which is triggered by TWS on file arrival
-  Data is validated using re-usable Spark program
-  validated files are moved to final folder and loaded into Hive table 
- if a file fails validation, then it will be moved to a different location (bad_file) in HDFS
- a log event will be sent to centralised logging framework with the details of the operation
- Spark also perfom derivation using MLLib
- this derived data gets loaded into hive 
********************************************
Data Consumption:
- Business user explore the transaction and agreegated data.
- this data pushed to terdata data for further consumption
- uses are given controled access to hive tables and columns 
- fine grained access is managed using Apache Ranger.
- Metadata management and data lineage within hadoop is managed using Apache Atlas.
